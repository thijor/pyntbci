{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Getting started\n\nThis getting started tutorial shows an example of how to use the PyntBCI library for analysing code-modulated responses.\nThis tutorial makes use of a small dataset of EEG data, recorded from one participant during a single session. This data\nis already minimally preprocessed following a spectral filter with a band-pass at 2-30 Hz and a downsample from 2 kHz to\n240 Hz. The stimuli that were shown are a circularly shifted modulated Gold code (shift register length 6, shift between\nstimuli 2 bits), presented at 60 Hz. The participant focused on each of the 32 stimuli once in a 4x8 matrix speller,\nwhere each presentation lasted 4.2 seconds (2 code cycles) after a 0.8 second cue.\n\nIn this notebook, the reconvolution CCA (rCCA) method for decoding EEG is demonstrated, see [1]_ and [2]_. Additionally,\nrCCA is compared to eCCA, the so-called reference pipeline as discussed in the c-VEP review [3]_.\n\n## References\n.. [1] Thielen, J., Marsman, P., Farquhar, J., & Desain, P. (2021). From full calibration to zero training for a\n       code-modulated visual evoked potentials for brain\u2013computer interface. Journal of Neural Engineering, 18(5),\n       056007. DOI: https://doi.org/10.1088/1741-2552/abecef\n.. [2] Thielen, J., van den Broek, P., Farquhar, J., & Desain, P. (2015). Broad-Band visually evoked potentials:\n       re(con)volution in brain-computer interfacing. PLOS ONE, 10(7), e0133797.\n       DOI: https://doi.org/10.1371/journal.pone.0133797\n.. [3] Mart\u00ednez-Cagigal, V., Thielen, J., Santamaria-Vazquez, E., P\u00e9rez-Velasco, S., Desain, P., & Hornero, R. (2021).\n       Brain\u2013computer interfaces based on code-modulated visual evoked potentials (c-VEP): A literature review. Journal\n       of Neural Engineering, 18(6), 061002. DOI: https://doi.org/10.1088/1741-2552/ac38cf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn\n\nimport pyntbci\n\nseaborn.set_context(\"paper\", font_scale=1.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The data\nThe dataset consists of: (1) The EEG data X that is a matrix of k trials, c channels, and m samples; (2) The labels y\nthat is a vector of k trials; (3) The pseudo-random noise-codes V that is a matrix of stimuli with n classes and m\nsamples. Note, the stimuli are upsampled to the EEG sampling frequency and contain only one stimulus-cycle. During a\ntrial, however, the stimuli were repeated 2 times (2 stimulus cycles).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Path to pyntbci (to read the tutorial data and standard cap files)\npath = os.path.join(os.path.dirname(pyntbci.__file__))\n\n# Load tutorial data\ntmp = np.load(os.path.join(path, \"data\", \"tutorial.npz\"))\nX = tmp[\"X\"]\ny = tmp[\"y\"]\nV = tmp[\"V\"]\nfs = tmp[\"fs\"]\nfr = 60\nprint(\"X\", X.shape, \"(trials x channels x samples)\")  # EEG\nprint(\"y\", y.shape, \"(trials)\")  # labels\nprint(\"V\", V.shape, \"(classes, samples)\")  # codes\nprint(\"fs\", fs, \"Hz\")  # sampling frequency\nprint(\"fr\", fr, \"Hz\")  # presentation rate\n\n# Extract data dimensions\nn_trials, n_channels, n_samples = X.shape\nn_classes = V.shape[0]\n\n# Read cap file\ncapfile = os.path.join(path, \"capfiles\", \"biosemi64.loc\")\nwith open(capfile, \"r\") as fid:\n    channels = []\n    for line in fid.readlines():\n        channels.append(line.split(\"\\t\")[-1].strip())\nprint(\"Channels:\", \", \".join(channels))\n\n# Visualize EEG data\ni_trial = 0  # the trial to visualize\nplt.figure(figsize=(15, 15))\nplt.plot(np.arange(0, n_samples) / fs, 25e-6 * np.arange(n_channels) + X[i_trial, :, :].T)\nplt.xlim([0, 1])  # limit to 1 second EEG data\nplt.yticks(25e-6 * np.arange(n_channels), channels)\nplt.xlabel(\"time [sec]\")\nplt.ylabel(\"channel\")\nplt.title(f\"Single-trial multi-channel EEG time-series (trial {i_trial})\")\nplt.tight_layout()\n\n# Visualize labels\nplt.figure(figsize=(15, 3))\nhist = np.histogram(y, bins=np.arange(n_classes + 1))[0]\nplt.bar(np.arange(n_classes), hist)\nplt.xticks(np.arange(n_classes))\nplt.xlabel(\"label\")\nplt.ylabel(\"count\")\nplt.title(\"Single-trial labels\")\nplt.tight_layout()\n\n# Visualize stimuli\nVup = V.repeat(20, axis=1)  # upsample to better visualize the sharp edges\nplt.figure(figsize=(15, 8))\nplt.plot(np.arange(Vup.shape[1]) / (20 * fs), 2 * np.arange(n_classes) + Vup.T)\nfor i in range(1 + int(V.shape[1] / (fs / fr))):\n    plt.axvline(i / fr, c=\"k\", alpha=0.1)\nplt.yticks(2 * np.arange(n_classes), np.arange(n_classes))\nplt.xlabel(\"time [sec]\")\nplt.ylabel(\"code\")\nplt.title(\"Code time-series\")\nplt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The event matrix\nThe first step for reconvolution is to find within the sequences the repetitive events. This can be imposed \"manually\"\nby choosing the event definition that we believe the brain responds to. Here, the so-called \"duration\" event is used,\nwhich marks the length of a flash as the important piece of information. As the sequences in this dataset were\nmodulated, there are only two events: a short and a long flash. Additionally, a third event is added that will account\nfor the onset of a trial, during which all of a sudden the screen started flashing. The event matrix is a matrix of n\nclasses, e events, and m samples.\n\nPlease, note that more event definitions exist, which can be explored with the `event` variable of `rCCA`. For\ninstance, `event=\"contrast\"` is a useful event definition as well, which looks at rising and falling edges,\ngeneralising over the length of a flash.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Create event matrix\nE, events = pyntbci.utilities.event_matrix(V, event=\"duration\", onset_event=True)\nprint(\"E:\", E.shape, \"(classes x events x samples)\")\nprint(\"Events:\", \", \".join([str(event) for event in events]))\n\n# Visualize event time-series\ni_class = 0  # the class to visualize\nfig, ax = plt.subplots(1, 1, figsize=(15, 3))\npyntbci.plotting.eventplot(V[i_class, ::int(fs/fr)], E[i_class, :, ::int(fs/fr)], fs=fr, ax=ax, events=events)\nax.set_title(f\"Event time-series (code {i_class})\")\nplt.tight_layout()\n\n# Visualize event matrix\ni_class = 0\nplt.figure(figsize=(15, 3))\nplt.imshow(E[i_class, :, :], cmap=\"gray\")\nplt.gca().set_aspect(10)\nplt.xticks(np.arange(0, E.shape[2], 60), np.arange(0, E.shape[2], 60) / fs)\nplt.yticks(np.arange(E.shape[1]), events)\nplt.xlabel(\"time [sec]\")\nplt.title(f\"Event matrix (class {i_class})\")\nplt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The structure matrix\nThe second step for reconvolution is to model the expected responses associated to each of the events and their\noverlap. This is done in the so-called structure matrix (or design matrix). The structure matrix is essentially a\nToeplitz version of the event matrix. It allows to model the c-VEP as the dot product of r (the transient response to\nan event) and M (the structure matrix for a specific class) for the ith class. The structure matrix is a matrix of n\nclasses, l response samples, and m samples.\n\nAn important parameter here is the `encoding_length` argument. An easy abstraction is to assume the same length for\nthe responses to each of the events. However, one could also set different lengths for each of the events.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Create structure matrix\nencoding_length = int(0.3 * fs)  # 300 ms responses\nM = pyntbci.utilities.encoding_matrix(E, encoding_length)\nprint(\"M: shape:\", M.shape, \"(classes x encoding_length*events x samples)\")\n\n# Plot structure matrix\ni_class = 0  # the class to visualize\nplt.figure(figsize=(15, 6))\nplt.imshow(M[i_class, :, :], cmap=\"gray\")\nplt.xticks(np.arange(0, M.shape[2], 60), np.arange(0, M.shape[2], 60) / fs)\nplt.yticks(np.arange(0, E.shape[1] * encoding_length, 12), np.tile(np.arange(0, encoding_length, 12) / fs, E.shape[1]))\nplt.xlabel(\"time [sec]\")\nplt.ylabel(events[::-1])\nplt.title(f\"Structure matrix (class {i_class})\")\nplt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reconvolution CCA\nThe full reconvolution CCA (rCCA) pipeline is implemented as a scikit-learn compatible class in PyntBCI in\n`pyntbci.classifiers.rCCA`. All it needs are the binary sequences `stimulus`, the sampling frequency `fs`, the event\ndefinition `event`, the transient response size `encoding_length` and whether or not to include an event for the onset\nof a trial `onset_event`.\n\nWhen calling `rCCA.fit(X, y)` with training data `X` and labels `y`, the full decomposition is performed to obtain\nspatial filters `rCCA.w_` and temporal filter `rCCA.r_`.\n\nPlease note that the transient responses are concatenated in this temporal filter `rCCA.r_`. One can use\n`rCCA.events_` to disentangle these and find which response is associated to which event.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Perform CCA decomposition with duration event\nencoding_length = 0.3  # 300 ms responses\nrcca = pyntbci.classifiers.rCCA(stimulus=V, fs=fs, event=\"duration\", encoding_length=encoding_length, onset_event=True)\nrcca.fit(X, y)\nprint(\"w: \", rcca.w_.shape, \"(channels)\")\nprint(\"r: \", rcca.r_.shape, \"(encoding_length*events)\")\n\n# Plot CCA filters\nfig, ax = plt.subplots(1, 2, figsize=(15, 3))\npyntbci.plotting.topoplot(rcca.w_, capfile, ax=ax[0])\nax[0].set_title(\"Spatial filter\")\ntmp = np.reshape(rcca.r_, (len(rcca.events_), -1))\nfor i in range(len(rcca.events_)):\n    ax[1].plot(np.arange(int(encoding_length * fs)) / fs, tmp[i, :])\nax[1].legend(rcca.events_)\nax[1].set_xlabel(\"time [sec]\")\nax[1].set_ylabel(\"amplitude [a.u.]\")\nax[1].set_title(\"Transient responses\")\nfig.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cross-validation\nTo perform decoding, one can call `rCCA.fit(X_trn, y_trn)` on training data `X_trn` and labels `y_trn` and\n`rCCA.predict(X_tst)` on testing data `X_tst`. In this section, a chronological cross-validation is setup to evaluate\nthe performance of rCCA.\n\nAdditionally, a second classifier is introduced, `eCCA`, which is the so-called \"reference\" method for c-VEP decoding.\nInstead of using reconvolution for template generation (rCCA), eCCA computes templates by computing average responses\nto repeated trials. As in this dataset a single circularly shifted code was used, we can compute one template for this\ncode, and circularly shift it to generate templates for all other classes. Therefore, eCCA requires a `lags` parameter\nthat specifies the relationship between the different classes.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Chronological cross-validation\nn_folds = 4\nn_trials = int(X.shape[0] / n_folds)\nfolds = np.repeat(np.arange(n_folds), n_trials)\n\n# Loop folds\naccuracy = np.zeros((2, n_folds))\nfor i_fold in range(n_folds):\n    # Split data to train and test set\n    X_trn, y_trn = X[folds != i_fold, ...], y[folds != i_fold]\n    X_tst, y_tst = X[folds == i_fold, ...], y[folds == i_fold]\n\n    # rCCA\n    rcca = pyntbci.classifiers.rCCA(stimulus=V, fs=fs, event=\"contrast\", encoding_length=0.3, onset_event=True)\n    rcca.fit(X_trn, y_trn)\n    yh_tst = rcca.predict(X_tst)\n    accuracy[0, i_fold] = np.mean(yh_tst == y_tst)\n\n    # eCCA\n    ecca = pyntbci.classifiers.eCCA(lags=np.arange(0, 2 * 63, 4) / 60, fs=fs, cycle_size=2 * 63 / 60)\n    ecca.fit(X_trn, y_trn)\n    yh_tst = ecca.predict(X_tst)\n    accuracy[1, i_fold] = np.mean(yh_tst == y_tst)\n\n# Plot accuracy (over folds)\nplt.figure(figsize=(15, 3))\nplt.bar(-0.2 + np.arange(n_folds), accuracy[0, :], 0.4, label=\"rCCA\")\nplt.bar(0.2 + np.arange(n_folds), accuracy[1, :], 0.4, label=\"eCCA\")\nplt.axhline(1 / n_classes, color=\"k\", linestyle=\"--\", label=\"chance\", alpha=0.5)\nplt.xticks(np.arange(n_folds))\nplt.xlabel(\"(test) fold\")\nplt.ylabel(\"accuracy\")\nplt.legend()\nplt.title(\"Chronological cross-validation\")\nplt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learning curve\nWhen comparing eCCA and rCCA, one can appreciate that rCCA typically requires less data than eCCA. The reason for this\nis that rCCA reduce the number of free parameters to those of the transient responses instead of the full c-VEP, which\nat the same time allows to increase the amount of data to perform a kind of average over. This can be observed in the\nso-called learning curve, which shows the performance as a function of the amount of training data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Chronological cross-validation\nn_folds = 4\nn_trials = int(X.shape[0] / n_folds)\nfolds = np.repeat(np.arange(n_folds), n_trials)\n\n# Loop folds\naccuracy = np.zeros((2, n_trials * (n_folds - 1), n_folds))\nfor i_fold in range(n_folds):\n\n    # Split data to train and test set\n    X_trn, y_trn = X[folds != i_fold, ...], y[folds != i_fold]\n    X_tst, y_tst = X[folds == i_fold, ...], y[folds == i_fold]\n\n    # Loop trials for the learning curve\n    for i_trial in range(n_trials * (n_folds - 1)):\n        # rCCA\n        rcca = pyntbci.classifiers.rCCA(stimulus=V, fs=fs, event=\"duration\", encoding_length=0.3, onset_event=True)\n        rcca.fit(X_trn[:1 + i_trial, ...], y_trn[:1 + i_trial])\n        yh_tst = rcca.predict(X_tst)\n        accuracy[0, i_trial, i_fold] = np.mean(yh_tst == y_tst)\n\n        # eCCA\n        ecca = pyntbci.classifiers.eCCA(lags=np.arange(0, 2 * 63, 4) / 60, fs=fs, cycle_size=63 / 60)\n        ecca.fit(X_trn[:1 + i_trial, ...], y_trn[:1 + i_trial])\n        yh_tst = ecca.predict(X_tst)\n        accuracy[1, i_trial, i_fold] = np.mean(yh_tst == y_tst)\n\n# Plot learning curve\nplt.figure(figsize=(15, 3))\navg = accuracy[0, ...].mean(axis=-1)\nstd = accuracy[0, ...].std(axis=-1)\nplt.plot(np.arange(n_trials * (n_folds - 1)), avg, label=\"rCCA\")\nplt.fill_between(np.arange(n_trials * (n_folds - 1)), avg + std, avg - std, alpha=0.2)\navg = accuracy[1, ...].mean(axis=-1)\nstd = accuracy[1, ...].std(axis=-1)\nplt.plot(np.arange(n_trials * (n_folds - 1)), avg, label=\"eCCA\")\nplt.fill_between(np.arange(n_trials * (n_folds - 1)), avg + std, avg - std, alpha=0.2)\nplt.axhline(1 / n_classes, color=\"k\", linestyle=\"--\", label=\"chance\", alpha=0.5)\nplt.xlabel(\"train trials [#]\")\nplt.ylabel(\"accuracy\")\nplt.legend()\nplt.title(\"Learning curve\")\nplt.tight_layout()\n\n# plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}