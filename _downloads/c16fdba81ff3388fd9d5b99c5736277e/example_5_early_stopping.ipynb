{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Early stopping\nThis script shows how to use early stopping from PyntBCI for decoding c-VEP data. Early stopping refers to determining\nwhen to stop the processing or decoding of a trial based on the reliability of the input data. Early stopping may be of\ntwo kinds: static stopping and dynamic stopping. In static stopping, an optimal fixes stopping time is learned, while\nin dynamic stopping the optimal stopping time depends on reaching a certain criterion, which may naturally lead to a\nvariable stopping time.\n\nThe data used in this script come from Thielen et al. (2021), see references [1]_ and [2]_.\n\n## References\n.. [1] Thielen et al. (2021) From full calibration to zero training for a code-modulated visual evoked potentials brain\n       computer interface. DOI: https://doi.org/10.34973/9txv-z787\n.. [2] Thielen, J., Marsman, P., Farquhar, J., & Desain, P. (2021). From full calibration to zero training for a\n       code-modulated visual evoked potentials for brain\u2013computer interface. Journal of Neural Engineering, 18(5),\n       056007. DOI: https://doi.org/10.1088/1741-2552/abecef\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn\n\nimport pyntbci\n\nseaborn.set_context(\"paper\", font_scale=1.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set the data path\nThe cell below specifies where the dataset has been downloaded to. Please, make sure it is set correctly according to\nthe specification of your device. If none of the folder structures in the dataset were changed, the cells below should\nwork just as fine.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "path = os.path.join(os.path.dirname(pyntbci.__file__))  # path to the dataset\nsubject = \"sub-01\"  # the subject to analyse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The data\nThe dataset consists of (1) the EEG data X that is a matrix of k trials, c channels, and m samples; (2) the labels y\nthat is a vector of k trials; (3) the pseudo-random noise-codes V that is a matrix of n classes and m samples. Note,\nthe codes are upsampled to match the EEG sampling frequency and contain only one code-cycle.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Load data\nfn = os.path.join(path, \"data\", f\"thielen2021_{subject}.npz\")\ntmp = np.load(fn)\nX = tmp[\"X\"]\ny = tmp[\"y\"]\nV = tmp[\"V\"]\nfs = int(tmp[\"fs\"])\nfr = 60\nprint(\"X\", X.shape, \"(trials x channels x samples)\")  # EEG\nprint(\"y\", y.shape, \"(trials)\")  # labels\nprint(\"V\", V.shape, \"(classes, samples)\")  # codes\nprint(\"fs\", fs, \"Hz\")  # sampling frequency\nprint(\"fr\", fr, \"Hz\")  # presentation rate\n\n# Extract data dimensions\nn_trials, n_channels, n_samples = X.shape\nn_classes = V.shape[0]\n\n# Read cap file\ncapfile = os.path.join(path, \"capfiles\", \"thielen8.loc\")\nwith open(capfile, \"r\") as fid:\n    channels = []\n    for line in fid.readlines():\n        channels.append(line.split(\"\\t\")[-1].strip())\nprint(\"Channels:\", \", \".join(channels))\n\n# ##\n# Settings\n# --------\n# Some general settings for the following sections\n\n# Set trial duration\ntrial_time = 4.2  # limit trials to a certain duration in seconds\ninter_trial_time = 1.0  # ITI in seconds for computing ITR\nn_samples = int(trial_time * fs)\n\n# Setup rCCA\nencoding_length = 0.3  # seconds\nonset_event = True  # an event modeling the onset of a trial\nevent = \"refe\"\n\n# Set size of increments of trials\nsegment_time = 0.1  # seconds\nn_segments = int(trial_time / segment_time)\n\n# Set chronological cross-validation\nn_folds = 5\nfolds = np.repeat(np.arange(n_folds), int(n_trials / n_folds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Maximum accuracy static stopping\nThe \"maximum accuracy\" method is a static stopping method that learns one stopping time that given some training data\nreaches the maximum classification accuracy. During testing, all trials will stop as soon as that time is reached,\nhence static stopping.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Loop folds\naccuracy_max_acc = np.zeros(n_folds)\nduration_max_acc = np.zeros(n_folds)\nfor i_fold in range(n_folds):\n\n    # Split data to train and valid set\n    X_trn, y_trn = X[folds != i_fold, :, :n_samples], y[folds != i_fold]\n    X_tst, y_tst = X[folds == i_fold, :, :n_samples], y[folds == i_fold]\n\n    # Train template-matching classifier\n    rcca = pyntbci.classifiers.rCCA(stimulus=V, fs=fs, event=event, encoding_length=encoding_length,\n                                    onset_event=onset_event, score_metric=\"correlation\")\n    max_acc = pyntbci.stopping.CriterionStopping(rcca, segment_time, fs, criterion=\"accuracy\", optimization=\"max\")\n    max_acc.fit(X_trn, y_trn)\n\n    # Loop segments\n    yh_tst = np.zeros(X_tst.shape[0])\n    dur_tst = np.zeros(X_tst.shape[0])\n    for i_segment in range(n_segments):\n\n        # Apply template-matching classifier\n        tmp = max_acc.predict(X_tst[:, :, :int((1 + i_segment) * segment_time * fs)])\n\n        # Check stopped\n        idx = np.logical_and(tmp >= 0, dur_tst == 0)\n        yh_tst[idx] = tmp[idx]\n        dur_tst[idx] = (1 + i_segment) * segment_time\n        if np.all(dur_tst > 0):\n            break\n\n    # Compute accuracy\n    accuracy_max_acc[i_fold] = np.mean(yh_tst == y_tst)\n    duration_max_acc[i_fold] = np.mean(dur_tst)\n\n# Compute ITR\nitr_max_acc = pyntbci.utilities.itr(n_classes, accuracy_max_acc, duration_max_acc + inter_trial_time)\n\n# Plot accuracy (over folds)\nfig, ax = plt.subplots(3, 1, figsize=(15, 8), sharex=True)\nax[0].bar(np.arange(n_folds), accuracy_max_acc)\nax[0].hlines(np.mean(accuracy_max_acc), -.5, n_folds - 0.5, linestyle='--', color=\"k\", alpha=0.5)\nax[1].bar(np.arange(n_folds), duration_max_acc)\nax[1].hlines(np.mean(duration_max_acc), -.5, n_folds - 0.5, linestyle='--', color=\"k\", alpha=0.5)\nax[2].bar(np.arange(n_folds), itr_max_acc)\nax[2].hlines(np.mean(itr_max_acc), -.5, n_folds - 0.5, linestyle='--', color=\"k\", alpha=0.5)\nax[2].set_xlabel(\"(test) fold\")\nax[0].set_ylabel(\"accuracy\")\nax[1].set_ylabel(\"duration [s]\")\nax[2].set_ylabel(\"itr [bits/min]\")\nax[0].set_title(f\"Maximum accuracy early stopping: avg acc {accuracy_max_acc.mean():.2f} | \" +\n                f\"avg dur {duration_max_acc.mean():.2f} | avg itr {itr_max_acc.mean():.1f}\")\n\n# Print accuracy (average and standard deviation over folds)\nprint(\"Maximum accuracy:\")\nprint(f\"\\tAccuracy: avg={accuracy_max_acc.mean():.2f} with std={accuracy_max_acc.std():.2f}\")\nprint(f\"\\tDuration: avg={duration_max_acc.mean():.2f} with std={duration_max_acc.std():.2f}\")\nprint(f\"\\tITR: avg={itr_max_acc.mean():.1f} with std={itr_max_acc.std():.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Maximum ITR static stopping\nThe \"maximum ITR\" method is a static stopping method that learns one stopping time that given some training data\nreaches the maximum information-transfer rate (ITR). During testing, all trials will stop as soon as that time is\nreached, hence static stopping.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Loop folds\naccuracy_max_itr = np.zeros(n_folds)\nduration_max_itr = np.zeros(n_folds)\nfor i_fold in range(n_folds):\n\n    # Split data to train and valid set\n    X_trn, y_trn = X[folds != i_fold, :, :n_samples], y[folds != i_fold]\n    X_tst, y_tst = X[folds == i_fold, :, :n_samples], y[folds == i_fold]\n\n    # Train template-matching classifier\n    rcca = pyntbci.classifiers.rCCA(stimulus=V, fs=fs, event=event, encoding_length=encoding_length,\n                                    onset_event=onset_event, score_metric=\"correlation\")\n    max_itr = pyntbci.stopping.CriterionStopping(rcca, segment_time, fs, criterion=\"itr\", optimization=\"max\",\n                                                 smooth_width=0.3)\n    max_itr.fit(X_trn, y_trn)\n\n    # Loop segments\n    yh_tst = np.zeros(X_tst.shape[0])\n    dur_tst = np.zeros(X_tst.shape[0])\n    for i_segment in range(n_segments):\n\n        # Apply template-matching classifier\n        tmp = max_itr.predict(X_tst[:, :, :int((1 + i_segment) * segment_time * fs)])\n\n        # Check stopped\n        idx = np.logical_and(tmp >= 0, dur_tst == 0)\n        yh_tst[idx] = tmp[idx]\n        dur_tst[idx] = (1 + i_segment) * segment_time\n        if np.all(dur_tst > 0):\n            break\n\n    # Compute accuracy\n    accuracy_max_itr[i_fold] = np.mean(yh_tst == y_tst)\n    duration_max_itr[i_fold] = np.mean(dur_tst)\n\n# Compute ITR\nitr_max_itr = pyntbci.utilities.itr(n_classes, accuracy_max_itr, duration_max_itr + inter_trial_time)\n\n# Plot accuracy (over folds)\nfig, ax = plt.subplots(3, 1, figsize=(15, 8), sharex=True)\nax[0].bar(np.arange(n_folds), accuracy_max_itr)\nax[0].hlines(np.mean(accuracy_max_itr), -.5, n_folds - 0.5, linestyle='--', color=\"k\", alpha=0.5)\nax[1].bar(np.arange(n_folds), duration_max_itr)\nax[1].hlines(np.mean(duration_max_itr), -.5, n_folds - 0.5, linestyle='--', color=\"k\", alpha=0.5)\nax[2].bar(np.arange(n_folds), itr_max_itr)\nax[2].hlines(np.mean(itr_max_itr), -.5, n_folds - 0.5, linestyle='--', color=\"k\", alpha=0.5)\nax[2].set_xlabel(\"(test) fold\")\nax[0].set_ylabel(\"accuracy\")\nax[1].set_ylabel(\"duration [s]\")\nax[2].set_ylabel(\"itr [bits/min]\")\nax[0].set_title(f\"Maximum ITR early stopping: avg acc {accuracy_max_itr.mean():.2f} | \" +\n                f\"avg dur {duration_max_itr.mean():.2f} | avg itr {itr_max_itr.mean():.1f}\")\n\n# Print accuracy (average and standard deviation over folds)\nprint(\"Maximum ITR:\")\nprint(f\"\\tAccuracy: avg={accuracy_max_itr.mean():.2f} with std={accuracy_max_itr.std():.2f}\")\nprint(f\"\\tDuration: avg={duration_max_itr.mean():.2f} with std={duration_max_itr.std():.2f}\")\nprint(f\"\\tITR: avg={itr_max_itr.mean():.1f} with std={itr_max_itr.std():.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Targeted accuracy static stopping\nThe \"targeted accuracy\" method is a static stopping method that learns one stopping time that given some training data\nreaches a preset targeted accuracy. During testing, all trials will stop as soon as that time is reached, hence static\nstopping.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Target accuracy\ntarget_p = 0.90 ** (1 / n_segments)\n\n# Loop folds\naccuracy_tgt_acc = np.zeros(n_folds)\nduration_tgt_acc = np.zeros(n_folds)\nfor i_fold in range(n_folds):\n\n    # Split data to train and valid set\n    X_trn, y_trn = X[folds != i_fold, :, :n_samples], y[folds != i_fold]\n    X_tst, y_tst = X[folds == i_fold, :, :n_samples], y[folds == i_fold]\n\n    # Train template-matching classifier\n    rcca = pyntbci.classifiers.rCCA(stimulus=V, fs=fs, event=event, encoding_length=encoding_length,\n                                    onset_event=onset_event, score_metric=\"correlation\")\n    tgt_acc = pyntbci.stopping.CriterionStopping(rcca, segment_time, fs, criterion=\"accuracy\", optimization=\"target\",\n                                                 target=target_p)\n    tgt_acc.fit(X_trn, y_trn)\n\n    # Loop segments\n    yh_tst = np.zeros(X_tst.shape[0])\n    dur_tst = np.zeros(X_tst.shape[0])\n    for i_segment in range(n_segments):\n\n        # Apply template-matching classifier\n        tmp = tgt_acc.predict(X_tst[:, :, :int((1 + i_segment) * segment_time * fs)])\n\n        # Check stopped\n        idx = np.logical_and(tmp >= 0, dur_tst == 0)\n        yh_tst[idx] = tmp[idx]\n        dur_tst[idx] = (1 + i_segment) * segment_time\n        if np.all(dur_tst > 0):\n            break\n\n    # Compute accuracy\n    accuracy_tgt_acc[i_fold] = np.mean(yh_tst == y_tst)\n    duration_tgt_acc[i_fold] = np.mean(dur_tst)\n\n# Compute ITR\nitr_tgt_acc = pyntbci.utilities.itr(n_classes, accuracy_tgt_acc, duration_tgt_acc + inter_trial_time)\n\n# Plot accuracy (over folds)\nfig, ax = plt.subplots(3, 1, figsize=(15, 8), sharex=True)\nax[0].bar(np.arange(n_folds), accuracy_tgt_acc)\nax[0].hlines(np.mean(accuracy_tgt_acc), -.5, n_folds - 0.5, linestyle='--', color=\"k\", alpha=0.5)\nax[1].bar(np.arange(n_folds), duration_tgt_acc)\nax[1].hlines(np.mean(duration_tgt_acc), -.5, n_folds - 0.5, linestyle='--', color=\"k\", alpha=0.5)\nax[2].bar(np.arange(n_folds), itr_tgt_acc)\nax[2].hlines(np.mean(itr_tgt_acc), -.5, n_folds - 0.5, linestyle='--', color=\"k\", alpha=0.5)\nax[2].set_xlabel(\"(test) fold\")\nax[0].set_ylabel(\"accuracy\")\nax[1].set_ylabel(\"duration [s]\")\nax[2].set_ylabel(\"itr [bits/min]\")\nax[0].set_title(f\"Targeted accuracy early stopping: avg acc {accuracy_tgt_acc.mean():.2f} | \" +\n                f\"avg dur {duration_tgt_acc.mean():.2f} | avg itr {itr_tgt_acc.mean():.1f}\")\n\n# Print accuracy (average and standard deviation over folds)\nprint(\"Targeted accuracy:\")\nprint(f\"\\tAccuracy: avg={accuracy_tgt_acc.mean():.2f} with std={accuracy_tgt_acc.std():.2f}\")\nprint(f\"\\tDuration: avg={duration_tgt_acc.mean():.2f} with std={duration_tgt_acc.std():.2f}\")\nprint(f\"\\tITR: avg={itr_tgt_acc.mean():.1f} with std={itr_tgt_acc.std():.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Margin dynamic stopping\nThe margin method learns threshold margins (i.e., the difference between the best and second-best score) to stop.\nThese margins are defined as such that a targeted accuracy is reached.\n\nReferences:\n\n.. [3] Thielen, J., van den Broek, P., Farquhar, J., & Desain, P. (2015). Broad-Band visually evoked potentials:\n       re(con)volution in brain-computer interfacing. PLOS ONE, 10(7), e0133797.\n       doi: https://doi.org/10.1371/journal.pone.0133797\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Target accuracy\ntarget_p = 0.90 ** (1 / n_segments)\n\n# Fit classifier\nrcca = pyntbci.classifiers.rCCA(stimulus=V, fs=fs, event=event, encoding_length=encoding_length,\n                                onset_event=onset_event, score_metric=\"correlation\")\nmargin = pyntbci.stopping.MarginStopping(rcca, segment_time=segment_time, fs=fs, target_p=target_p, max_time=trial_time)\nmargin.fit(X, y)\n\n# Plot dynamic stopping\nplt.figure(figsize=(15, 3))\nplt.plot(np.arange(1, 1 + margin.margins_.size) * segment_time, margin.margins_, c=\"k\")\nplt.xlabel(\"time [s]\")\nplt.ylabel(\"margin\")\nplt.title(\"Margin dynamic stopping\")\n\n# Loop folds\naccuracy_margin = np.zeros(n_folds)\nduration_margin = np.zeros(n_folds)\nfor i_fold in range(n_folds):\n\n    # Split data to train and valid set\n    X_trn, y_trn = X[folds != i_fold, :, :n_samples], y[folds != i_fold]\n    X_tst, y_tst = X[folds == i_fold, :, :n_samples], y[folds == i_fold]\n\n    # Train template-matching classifier\n    rcca = pyntbci.classifiers.rCCA(stimulus=V, fs=fs, event=event, encoding_length=encoding_length,\n                                    onset_event=onset_event, score_metric=\"correlation\")\n    margin = pyntbci.stopping.MarginStopping(rcca, segment_time=segment_time, fs=fs, target_p=target_p)\n    margin.fit(X_trn, y_trn)\n\n    # Loop segments\n    yh_tst = np.zeros(X_tst.shape[0])\n    dur_tst = np.zeros(X_tst.shape[0])\n    for i_segment in range(n_segments):\n\n        # Apply template-matching classifier\n        tmp = margin.predict(X_tst[:, :, :int((1 + i_segment) * segment_time * fs)])\n\n        # Check stopped\n        idx = np.logical_and(tmp >= 0, dur_tst == 0)\n        yh_tst[idx] = tmp[idx]\n        dur_tst[idx] = (1 + i_segment) * segment_time\n        if np.all(dur_tst > 0):\n            break\n\n    # Compute accuracy\n    accuracy_margin[i_fold] = np.mean(yh_tst == y_tst)\n    duration_margin[i_fold] = np.mean(dur_tst)\n\n# Compute ITR\nitr_margin = pyntbci.utilities.itr(n_classes, accuracy_margin, duration_margin + inter_trial_time)\n\n# Plot accuracy (over folds)\nfig, ax = plt.subplots(3, 1, figsize=(15, 8), sharex=True)\nax[0].bar(np.arange(n_folds), accuracy_margin)\nax[0].hlines(np.mean(accuracy_margin), -.5, n_folds - 0.5, linestyle='--', color=\"k\", alpha=0.5)\nax[1].bar(np.arange(n_folds), duration_margin)\nax[1].hlines(np.mean(duration_margin), -.5, n_folds - 0.5, linestyle='--', color=\"k\", alpha=0.5)\nax[2].bar(np.arange(n_folds), itr_margin)\nax[2].hlines(np.mean(itr_margin), -.5, n_folds - 0.5, linestyle='--', color=\"k\", alpha=0.5)\nax[2].set_xlabel(\"(test) fold\")\nax[0].set_ylabel(\"accuracy\")\nax[1].set_ylabel(\"duration [s]\")\nax[2].set_ylabel(\"itr [bits/min]\")\nax[0].set_title(f\"Margin dynamic stopping: avg acc {accuracy_margin.mean():.2f} | \" +\n                f\"avg dur {duration_margin.mean():.2f} | avg itr {itr_margin.mean():.1f}\")\n\n# Print accuracy (average and standard deviation over folds)\nprint(\"Margin:\")\nprint(f\"\\tAccuracy: avg={accuracy_margin.mean():.2f} with std={accuracy_margin.std():.2f}\")\nprint(f\"\\tDuration: avg={duration_margin.mean():.2f} with std={duration_margin.std():.2f}\")\nprint(f\"\\tITR: avg={itr_margin.mean():.1f} with std={itr_margin.std():.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Beta dynamic stopping\nThe beta method fits a beta distribution to the non-maximum scores (i.e., if correlation, then correlation+1)/2), and\ntests the probability of the maximum correlation to belong to that beta distribution.\n\nReferences:\n\n.. [4] Thielen, J., Marsman, P., Farquhar, J., & Desain, P. (2021). From full calibration to zero training for a\n       code-modulated visual evoked potentials for brain\u2013computer interface. Journal of Neural Engineering, 18(5),\n       056007. doi: http://doi.org/10.1088/1741-2552/abecef\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Target accuracy\ntarget_p = 0.90 ** (1 / n_segments)\n\n# Loop folds\naccuracy_beta = np.zeros(n_folds)\nduration_beta = np.zeros(n_folds)\nfor i_fold in range(n_folds):\n\n    # Split data to train and valid set\n    X_trn, y_trn = X[folds != i_fold, :, :n_samples], y[folds != i_fold]\n    X_tst, y_tst = X[folds == i_fold, :, :n_samples], y[folds == i_fold]\n\n    # Train template-matching classifier\n    rcca = pyntbci.classifiers.rCCA(stimulus=V, fs=fs, event=event, encoding_length=encoding_length,\n                                    onset_event=onset_event, score_metric=\"correlation\")\n    beta = pyntbci.stopping.DistributionStopping(rcca, segment_time=segment_time, fs=fs, target_p=target_p,\n                                                 distribution=\"beta\", max_time=trial_time)\n    beta.fit(X, y)\n\n    # Loop segments\n    yh_tst = np.zeros(X_tst.shape[0])\n    dur_tst = np.zeros(X_tst.shape[0])\n    for i_segment in range(n_segments):\n\n        # Apply template-matching classifier\n        tmp = beta.predict(X_tst[:, :, :int((1 + i_segment) * segment_time * fs)])\n\n        # Check stopped\n        idx = np.logical_and(tmp >= 0, dur_tst == 0)\n        yh_tst[idx] = tmp[idx]\n        dur_tst[idx] = (1 + i_segment) * segment_time\n        if np.all(dur_tst > 0):\n            break\n\n    # Compute accuracy\n    accuracy_beta[i_fold] = np.mean(yh_tst == y_tst)\n    duration_beta[i_fold] = np.mean(dur_tst)\n\n# Compute ITR\nitr_beta = pyntbci.utilities.itr(n_classes, accuracy_beta, duration_beta + inter_trial_time)\n\n# Plot accuracy (over folds)\nfig, ax = plt.subplots(3, 1, figsize=(15, 8), sharex=True)\nax[0].bar(np.arange(n_folds), accuracy_beta)\nax[0].hlines(np.mean(accuracy_beta), -.5, n_folds - 0.5, linestyle='--', color=\"k\", alpha=0.5)\nax[1].bar(np.arange(n_folds), duration_beta)\nax[1].hlines(np.mean(duration_beta), -.5, n_folds - 0.5, linestyle='--', color=\"k\", alpha=0.5)\nax[2].bar(np.arange(n_folds), itr_beta)\nax[2].hlines(np.mean(itr_beta), -.5, n_folds - 0.5, linestyle='--', color=\"k\", alpha=0.5)\nax[2].set_xlabel(\"(test) fold\")\nax[0].set_ylabel(\"accuracy\")\nax[1].set_ylabel(\"duration [s]\")\nax[2].set_ylabel(\"itr [bits/min]\")\nax[0].set_title(f\"Beta dynamic stopping: avg acc {accuracy_beta.mean():.2f} | \" +\n                f\"avg dur {duration_beta.mean():.2f} | avg itr {itr_beta.mean():.1f}\")\n\n# Print accuracy (average and standard deviation over folds)\nprint(\"Beta:\")\nprint(f\"\\tAccuracy: avg={accuracy_beta.mean():.2f} with std={accuracy_beta.std():.2f}\")\nprint(f\"\\tDuration: avg={duration_beta.mean():.2f} with std={duration_beta.std():.2f}\")\nprint(f\"\\tITR: avg={itr_beta.mean():.1f} with std={itr_beta.std():.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bayesian dynamic stopping (BDS0)\nThe Bayesian method fits Gaussian distributions for target and non-target responses, and calculates a stopping\nthreshold using these and a cost criterion. This method comes in three flavours: bds0, bds1, and bds2.\n\nReferences:\n\n.. [5] Ahmadi, S., Desain, P. & Thielen, J. (submitted) A Bayesian dynamic stopping method for evoked\nresponse brain-computer interfacing\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Cost ratio and target probabilities\ncr = 1.0\n\n# Fit classifier\nrcca = pyntbci.classifiers.rCCA(stimulus=V, fs=fs, event=event, encoding_length=encoding_length,\n                                onset_event=onset_event, score_metric=\"inner\")\nbayes = pyntbci.stopping.BayesStopping(rcca, segment_time=segment_time, fs=fs, cr=cr, max_time=trial_time)\nbayes.fit(X, y)\n\n# Plot dynamic stopping\nfig, ax = plt.subplots(2, 1, figsize=(15, 4), sharex=True)\nax[0].plot(np.arange(1, 1 + bayes.eta_.size) * segment_time, bayes.eta_, c=\"k\", label=\"eta\")\nax[0].plot(np.arange(1, 1 + bayes.eta_.size) * segment_time, bayes.alpha_ * bayes.b0_, \"--b\", label=\"b0\")\nax[0].plot(np.arange(1, 1 + bayes.eta_.size) * segment_time, bayes.alpha_ * bayes.b1_, \"--g\", label=\"b1\")\nax[0].plot(np.arange(1, 1 + bayes.eta_.size) * segment_time, bayes.alpha_ * bayes.b0_ - bayes.s0_, \"b\")\nax[0].plot(np.arange(1, 1 + bayes.eta_.size) * segment_time, bayes.alpha_ * bayes.b1_ - bayes.s1_, \"g\")\nax[0].plot(np.arange(1, 1 + bayes.eta_.size) * segment_time, bayes.alpha_ * bayes.b0_ + bayes.s0_, \"b\")\nax[0].plot(np.arange(1, 1 + bayes.eta_.size) * segment_time, bayes.alpha_ * bayes.b1_ + bayes.s1_, \"g\")\nax[0].legend()\nax[1].plot(np.arange(1, 1 + bayes.eta_.size) * segment_time, bayes.pf_, label=\"pf\")\nax[1].plot(np.arange(1, 1 + bayes.eta_.size) * segment_time, bayes.pm_, label=\"pm\")\nax[1].legend()\nax[1].set_xlabel(\"time [s]\")\nax[0].set_title(\"Bayesian dynamic stopping\")\n\n# Loop folds\naccuracy_bds0 = np.zeros(n_folds)\nduration_bds0 = np.zeros(n_folds)\nfor i_fold in range(n_folds):\n\n    # Split data to train and valid set\n    X_trn, y_trn = X[folds != i_fold, :, :n_samples], y[folds != i_fold]\n    X_tst, y_tst = X[folds == i_fold, :, :n_samples], y[folds == i_fold]\n\n    # Train template-matching classifier\n    rcca = pyntbci.classifiers.rCCA(stimulus=V, fs=fs, event=event, encoding_length=encoding_length,\n                                    onset_event=onset_event, score_metric=\"inner\")\n    bayes = pyntbci.stopping.BayesStopping(rcca, segment_time=segment_time, fs=fs, method=\"bds0\", cr=cr,\n                                           max_time=trial_time)\n    bayes.fit(X_trn, y_trn)\n\n    # Apply template-matching classifier\n    yh_tst = np.zeros(X_tst.shape[0])\n    dur_tst = np.zeros(X_tst.shape[0])\n    for i_segment in range(n_segments):\n        tmp = bayes.predict(X_tst[:, :, :int((1 + i_segment) * segment_time * fs)])\n        idx = np.logical_and(tmp >= 0, dur_tst == 0)\n        yh_tst[idx] = tmp[idx]\n        dur_tst[idx] = (1 + i_segment) * segment_time\n        if np.all(dur_tst > 0):\n            break\n\n    # Compute accuracy\n    accuracy_bds0[i_fold] = np.mean(yh_tst == y_tst)\n    duration_bds0[i_fold] = np.mean(dur_tst)\n\n# Compute ITR\nitr_bds0 = pyntbci.utilities.itr(n_classes, accuracy_bds0, duration_bds0 + inter_trial_time)\n\n# Plot accuracy (over folds)\nfig, ax = plt.subplots(3, 1, figsize=(15, 8), sharex=True)\nax[0].bar(np.arange(n_folds), accuracy_bds0)\nax[0].hlines(np.mean(accuracy_bds0), -.5, n_folds - 0.5, linestyle='--', color=\"k\", alpha=0.5)\nax[1].bar(np.arange(n_folds), duration_bds0)\nax[1].hlines(np.mean(duration_bds0), -.5, n_folds - 0.5, linestyle='--', color=\"k\", alpha=0.5)\nax[2].bar(np.arange(n_folds), itr_bds0)\nax[2].hlines(np.mean(itr_bds0), -.5, n_folds - 0.5, linestyle='--', color=\"k\", alpha=0.5)\nax[2].set_xlabel(\"(test) fold\")\nax[0].set_ylabel(\"accuracy\")\nax[1].set_ylabel(\"duration [s]\")\nax[2].set_ylabel(\"itr [bits/min]\")\nax[0].set_title(f\"BDS0 dynamic stopping: avg acc {accuracy_bds0.mean():.2f} | \" +\n                f\"avg dur {duration_bds0.mean():.2f} | avg itr {itr_bds0.mean():.1f}\")\n\n# Print accuracy (average and standard deviation over folds)\nprint(\"BDS0:\")\nprint(f\"\\tAccuracy: avg={accuracy_bds0.mean():.2f} with std={accuracy_bds0.std():.2f}\")\nprint(f\"\\tDuration: avg={duration_bds0.mean():.2f} with std={duration_bds0.std():.2f}\")\nprint(f\"\\tITR: avg={itr_bds0.mean():.1f} with std={itr_bds0.std():.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bayesian dynamic stopping (BDS1)\nThe Bayesian method fits Gaussian distributions for target and non-target responses, and calculates a stopping\nthreshold using these and a cost criterion. This method comes in three flavours: bds0, bds1, and bds2.\n\nReferences:\n\n.. [6] Ahmadi, S., Desain, P. & Thielen, J. (submitted) A Bayesian dynamic stopping method for evoked\nresponse brain-computer interfacing\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Cost ratio and target probabilities\ncr = 1.0\ntarget_pf = 0.05\ntarget_pd = 0.80\n\n# Loop folds\naccuracy_bds1 = np.zeros(n_folds)\nduration_bds1 = np.zeros(n_folds)\nfor i_fold in range(n_folds):\n\n    # Split data to train and valid set\n    X_trn, y_trn = X[folds != i_fold, :, :n_samples], y[folds != i_fold]\n    X_tst, y_tst = X[folds == i_fold, :, :n_samples], y[folds == i_fold]\n\n    # Train template-matching classifier\n    rcca = pyntbci.classifiers.rCCA(stimulus=V, fs=fs, event=event, encoding_length=encoding_length,\n                                    onset_event=onset_event, score_metric=\"inner\")\n    bayes = pyntbci.stopping.BayesStopping(rcca, segment_time=segment_time, fs=fs, method=\"bds1\", cr=cr,\n                                           target_pf=target_pf, target_pd=target_pd, max_time=trial_time)\n    bayes.fit(X_trn, y_trn)\n\n    # Apply template-matching classifier\n    yh_tst = np.zeros(X_tst.shape[0])\n    dur_tst = np.zeros(X_tst.shape[0])\n    for i_segment in range(n_segments):\n        tmp = bayes.predict(X_tst[:, :, :int((1 + i_segment) * segment_time * fs)])\n        idx = np.logical_and(tmp >= 0, dur_tst == 0)\n        yh_tst[idx] = tmp[idx]\n        dur_tst[idx] = (1 + i_segment) * segment_time\n        if np.all(dur_tst > 0):\n            break\n\n    # Compute accuracy\n    accuracy_bds1[i_fold] = np.mean(yh_tst == y_tst)\n    duration_bds1[i_fold] = np.mean(dur_tst)\n\n# Compute ITR\nitr_bds1 = pyntbci.utilities.itr(n_classes, accuracy_bds1, duration_bds1 + inter_trial_time)\n\n# Plot accuracy (over folds)\nfig, ax = plt.subplots(3, 1, figsize=(15, 8), sharex=True)\nax[0].bar(np.arange(n_folds), accuracy_bds1)\nax[0].hlines(np.mean(accuracy_bds1), -.5, n_folds - 0.5, linestyle='--', color=\"k\", alpha=0.5)\nax[1].bar(np.arange(n_folds), duration_bds1)\nax[1].hlines(np.mean(duration_bds1), -.5, n_folds - 0.5, linestyle='--', color=\"k\", alpha=0.5)\nax[2].bar(np.arange(n_folds), itr_bds1)\nax[2].hlines(np.mean(itr_bds1), -.5, n_folds - 0.5, linestyle='--', color=\"k\", alpha=0.5)\nax[2].set_xlabel(\"(test) fold\")\nax[0].set_ylabel(\"accuracy\")\nax[1].set_ylabel(\"duration [s]\")\nax[2].set_ylabel(\"itr [bits/min]\")\nax[0].set_title(f\"BDS1 dynamic stopping: avg acc {accuracy_bds1.mean():.2f} | \" +\n                f\"avg dur {duration_bds1.mean():.2f} | avg itr {itr_bds1.mean():.1f}\")\n\n# Print accuracy (average and standard deviation over folds)\nprint(\"BDS1:\")\nprint(f\"\\tAccuracy: avg={accuracy_bds1.mean():.2f} with std={accuracy_bds1.std():.2f}\")\nprint(f\"\\tDuration: avg={duration_bds1.mean():.2f} with std={duration_bds1.std():.2f}\")\nprint(f\"\\tITR: avg={itr_bds1.mean():.1f} with std={itr_bds1.std():.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bayesian dynamic stopping (BDS2)\nThe Bayesian method fits Gaussian distributions for target and non-target responses, and calculates a stopping\nthreshold using these and a cost criterion. This method comes in three flavours: bds0, bds1, and bds2.\n\nReferences:\n\n.. [7] Ahmadi, S., Desain, P. & Thielen, J. (submitted) A Bayesian dynamic stopping method for evoked\nresponse brain-computer interfacing\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Cost ratio and target probabilities\ncr = 1.0\ntarget_pf = 0.05\ntarget_pd = 0.80\n\n# Loop folds\naccuracy_bds2 = np.zeros(n_folds)\nduration_bds2 = np.zeros(n_folds)\nfor i_fold in range(n_folds):\n\n    # Split data to train and valid set\n    X_trn, y_trn = X[folds != i_fold, :, :n_samples], y[folds != i_fold]\n    X_tst, y_tst = X[folds == i_fold, :, :n_samples], y[folds == i_fold]\n\n    # Train template-matching classifier\n    rcca = pyntbci.classifiers.rCCA(stimulus=V, fs=fs, event=event, encoding_length=encoding_length,\n                                    onset_event=onset_event, score_metric=\"inner\")\n    bayes = pyntbci.stopping.BayesStopping(rcca, segment_time=segment_time, fs=fs, method=\"bds2\", cr=cr,\n                                           target_pf=target_pf, target_pd=target_pd, max_time=trial_time)\n    bayes.fit(X_trn, y_trn)\n\n    # Apply template-matching classifier\n    yh_tst = np.zeros(X_tst.shape[0])\n    dur_tst = np.zeros(X_tst.shape[0])\n    for i_segment in range(n_segments):\n        tmp = bayes.predict(X_tst[:, :, :int((1 + i_segment) * segment_time * fs)])\n        idx = np.logical_and(tmp >= 0, dur_tst == 0)\n        yh_tst[idx] = tmp[idx]\n        dur_tst[idx] = (1 + i_segment) * segment_time\n        if np.all(dur_tst > 0):\n            break\n\n    # Compute accuracy\n    accuracy_bds2[i_fold] = np.mean(yh_tst == y_tst)\n    duration_bds2[i_fold] = np.mean(dur_tst)\n\n# Compute ITR\nitr_bds2 = pyntbci.utilities.itr(n_classes, accuracy_bds2, duration_bds2 + inter_trial_time)\n\n# Plot accuracy (over folds)\nfig, ax = plt.subplots(3, 1, figsize=(15, 8), sharex=True)\nax[0].bar(np.arange(n_folds), accuracy_bds2)\nax[0].hlines(np.mean(accuracy_bds2), -.5, n_folds - 0.5, linestyle='--', color=\"k\", alpha=0.5)\nax[1].bar(np.arange(n_folds), duration_bds2)\nax[1].hlines(np.mean(duration_bds2), -.5, n_folds - 0.5, linestyle='--', color=\"k\", alpha=0.5)\nax[2].bar(np.arange(n_folds), itr_bds2)\nax[2].hlines(np.mean(itr_bds2), -.5, n_folds - 0.5, linestyle='--', color=\"k\", alpha=0.5)\nax[2].set_xlabel(\"(test) fold\")\nax[0].set_ylabel(\"accuracy\")\nax[1].set_ylabel(\"duration [s]\")\nax[2].set_ylabel(\"itr [bits/min]\")\nax[0].set_title(f\"BDS2 dynamic stopping: avg acc {accuracy_bds2.mean():.2f} | \" +\n                f\"avg dur {duration_bds2.mean():.2f} | avg itr {itr_bds2.mean():.1f}\")\n\n# Print accuracy (average and standard deviation over folds)\nprint(\"BDS2:\")\nprint(f\"\\tAccuracy: avg={accuracy_bds2.mean():.2f} with std={accuracy_bds2.std():.2f}\")\nprint(f\"\\tDuration: avg={duration_bds2.mean():.2f} with std={duration_bds2.std():.2f}\")\nprint(f\"\\tITR: avg={itr_bds2.mean():.1f} with std={itr_bds2.std():.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Overall comparison\nComparison of the presented stopping methods. Note, each of these use default parameters that might need fine-tuning.\nAdditionally, the evaluation is performed on a single participant only.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Plot accuracy\nwidth = 0.8\nfig, ax = plt.subplots(3, 1, figsize=(15, 8), sharex=True)\nax[0].bar(0, accuracy_max_acc.mean(), width=width, yerr=accuracy_max_acc.std(), label=\"maxacc\")\nax[0].bar(1, accuracy_max_itr.mean(), width=width, yerr=accuracy_max_itr.std(), label=\"maxitr\")\nax[0].bar(2, accuracy_tgt_acc.mean(), width=width, yerr=accuracy_tgt_acc.std(), label=\"tgtacc\")\nax[0].bar(3, accuracy_margin.mean(), width=width, yerr=accuracy_margin.std(), label=\"margin\")\nax[0].bar(4, accuracy_beta.mean(), width=width, yerr=accuracy_beta.std(), label=\"beta\")\nax[0].bar(5, accuracy_bds0.mean(), width=width, yerr=accuracy_bds0.std(), label=\"bds0\")\nax[0].bar(6, accuracy_bds1.mean(), width=width, yerr=accuracy_bds1.std(), label=\"bds1\")\nax[0].bar(7, accuracy_bds2.mean(), width=width, yerr=accuracy_bds2.std(), label=\"bds2\")\nax[1].bar(0, duration_max_acc.mean(), width=width, yerr=duration_max_acc.std(), label=\"maxacc\")\nax[1].bar(1, duration_max_itr.mean(), width=width, yerr=duration_max_itr.std(), label=\"maxitr\")\nax[1].bar(2, duration_tgt_acc.mean(), width=width, yerr=duration_tgt_acc.std(), label=\"tgtacc\")\nax[1].bar(3, duration_margin.mean(), width=width, yerr=duration_margin.std(), label=\"margin\")\nax[1].bar(4, duration_beta.mean(), width=width, yerr=duration_beta.std(), label=\"beta\")\nax[1].bar(5, duration_bds0.mean(), width=width, yerr=duration_bds0.std(), label=\"bds0\")\nax[1].bar(6, duration_bds1.mean(), width=width, yerr=duration_bds1.std(), label=\"bds1\")\nax[1].bar(7, duration_bds2.mean(), width=width, yerr=duration_bds2.std(), label=\"bds2\")\nax[2].bar(0, itr_max_acc.mean(), width=width, yerr=itr_max_acc.std(), label=\"maxacc\")\nax[2].bar(1, itr_max_itr.mean(), width=width, yerr=itr_max_itr.std(), label=\"maxitr\")\nax[2].bar(2, itr_tgt_acc.mean(), width=width, yerr=itr_tgt_acc.std(), label=\"tgtacc\")\nax[2].bar(3, itr_margin.mean(), width=width, yerr=itr_margin.std(), label=\"margin\")\nax[2].bar(4, itr_beta.mean(), width=width, yerr=itr_beta.std(), label=\"beta\")\nax[2].bar(5, itr_bds0.mean(), width=width, yerr=itr_bds0.std(), label=\"bds0\")\nax[2].bar(6, itr_bds1.mean(), width=width, yerr=itr_bds1.std(), label=\"bds1\")\nax[2].bar(7, itr_bds2.mean(), width=width, yerr=itr_bds2.std(), label=\"bds2\")\nax[2].set_xticks(np.arange(8), [\"maxacc\", \"maxitr\", \"tgtacc\", \"margin\", \"beta\", \"bds0\", \"bds1\", \"bds2\"])\nax[2].set_xlabel(\"early stopping method\")\nax[0].set_ylabel(\"accuracy\")\nax[1].set_ylabel(\"duration [s]\")\nax[2].set_ylabel(\"itr [bits/min]\")\nax[1].legend(bbox_to_anchor=(1.0, 1.0))\nax[0].set_title(\"Comparison of early stopping methods averaged across folds\")\n\n# plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}