{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Early stopping\nThis script shows how to use early stopping from PyntBCI for decoding c-VEP data. Early stopping refers to determining\nwhen to stop the processing or decoding of a trial based on the reliability of the input data. Early stopping may be of\ntwo kinds: static stopping and dynamic stopping. In static stopping, an optimal fixes stopping time is learned, while\nin dynamic stopping the optimal stopping time depends on reaching a certain criterium, which may naturally lead to a\nvariable stopping time.\n\nThe data used in this script come from Thielen et al. (2021), see references [1]_ and [2]_.\n\n## References\n.. [1] Thielen et al. (2021) From full calibration to zero training for a code-modulated visual evoked potentials brain\n       computer interface. DOI: https://doi.org/10.34973/9txv-z787\n.. [2] Thielen, J., Marsman, P., Farquhar, J., & Desain, P. (2021). From full calibration to zero training for a\n       code-modulated visual evoked potentials for brain\u2013computer interface. Journal of Neural Engineering, 18(5),\n       056007. DOI: https://doi.org/10.1088/1741-2552/abecef\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn\n\nimport pyntbci\n\nseaborn.set_context(\"paper\", font_scale=1.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set the data path\nThe cell below specifies where the dataset has been downloaded to. Please, make sure it is set correctly according to\nthe specification of your device. If none of the folder structures in the dataset were changed, the cells below should\nwork just as fine.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "path = os.path.join(os.path.dirname(pyntbci.__file__))  # path to the dataset\nsubject = \"sub-01\"  # the subject to analyse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The data\nThe dataset consists of (1) the EEG data X that is a matrix of k trials, c channels, and m samples; (2) the labels y\nthat is a vector of k trials; (3) the pseudo-random noise-codes V that is a matrix of n classes and m samples. Note,\nthe codes are upsampled to match the EEG sampling frequency and contain only one code-cycle.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Load data\nfn = os.path.join(path, \"data\", f\"thielen2021_{subject}.npz\")\ntmp = np.load(fn)\nX = tmp[\"X\"]\ny = tmp[\"y\"]\nV = tmp[\"V\"]\nfs = int(tmp[\"fs\"])\nfr = 60\nprint(\"X\", X.shape, \"(trials x channels x samples)\")  # EEG\nprint(\"y\", y.shape, \"(trials)\")  # labels\nprint(\"V\", V.shape, \"(classes, samples)\")  # codes\nprint(\"fs\", fs, \"Hz\")  # sampling frequency\nprint(\"fr\", fr, \"Hz\")  # presentation rate\n\n# Extract data dimensions\nn_trials, n_channels, n_samples = X.shape\nn_classes = V.shape[0]\n\n# Read cap file\ncapfile = os.path.join(path, \"capfiles\", \"thielen8.loc\")\nwith open(capfile, \"r\") as fid:\n    channels = []\n    for line in fid.readlines():\n        channels.append(line.split(\"\\t\")[-1].strip())\nprint(\"Channels:\", \", \".join(channels))\n\n# Visualize EEG data\ni_trial = 0  # the trial to visualize\nplt.figure(figsize=(15, 5))\nplt.plot(np.arange(0, n_samples) / fs, 25e-6 * np.arange(n_channels) + X[i_trial, :, :].T)\nplt.xlim([0, 1])  # limit to 1 second EEG data\nplt.yticks(25e-6 * np.arange(n_channels), channels)\nplt.xlabel(\"time [sec]\")\nplt.ylabel(\"channel\")\nplt.title(f\"Single-trial multi-channel EEG time-series (trial {i_trial})\")\nplt.tight_layout()\n\n# Visualize labels\nplt.figure(figsize=(15, 3))\nhist = np.histogram(y, bins=np.arange(n_classes + 1))[0]\nplt.bar(np.arange(n_classes), hist)\nplt.xticks(np.arange(n_classes))\nplt.xlabel(\"label\")\nplt.ylabel(\"count\")\nplt.title(\"Single-trial labels\")\nplt.tight_layout()\n\n# Visualize stimuli\nVup = V.repeat(20, axis=1)  # upsample to better visualize the sharp edges\nplt.figure(figsize=(15, 8))\nplt.plot(np.arange(Vup.shape[1]) / (20 * fs), 2 * np.arange(n_classes) + Vup.T)\nfor i in range(1 + int(V.shape[1] / (fs / fr))):\n    plt.axvline(i / fr, c=\"k\", alpha=0.1)\nplt.yticks(2 * np.arange(n_classes), np.arange(n_classes))\nplt.xlabel(\"time [sec]\")\nplt.ylabel(\"code\")\nplt.title(\"Code time-series\")\nplt.tight_layout()\n\n# ##\n# Settings\n# --------\n# Some general settings for the following sections\n\n# Set trial duration\ntrial_time = 4.2  # limit trials to a certain duration in seconds\ninter_trial_time = 1.0  # ITI in seconds for computing ITR\nn_samples = int(trial_time * fs)\n\n# Setup rCCA\nencoding_length = 0.3\nonset_event = True\n\n# Set stopping\nsegment_time = 0.1\nn_segments = int(trial_time / segment_time)\n\n# Set chronological cross-validation\nn_folds = 5\nfolds = np.repeat(np.arange(n_folds), int(n_trials / n_folds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Maximum accuracy static stopping\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Loop folds\naccuracy_max_acc = np.zeros(n_folds)\nduration_max_acc = np.zeros(n_folds)\nfor i_fold in range(n_folds):\n\n    # Split data to train and valid set\n    X_trn, y_trn = X[folds != i_fold, :, :n_samples], y[folds != i_fold]\n    X_tst, y_tst = X[folds == i_fold, :, :n_samples], y[folds == i_fold]\n\n    # Train template-matching classifier\n    rcca = pyntbci.classifiers.rCCA(stimulus=V, fs=fs, event=\"duration\", encoding_length=encoding_length,\n                                    onset_event=onset_event, score_metric=\"correlation\")\n    max_acc = pyntbci.stopping.CriterionStopping(rcca, segment_time, fs, criterion=\"accuracy\", optimization=\"max\")\n    max_acc.fit(X_trn, y_trn)\n\n    # Loop segments\n    yh_tst = np.zeros(X_tst.shape[0])\n    dur_tst = np.zeros(X_tst.shape[0])\n    for i_segment in range(n_segments):\n\n        # Apply template-matching classifier\n        tmp = max_acc.predict(X_tst[:, :, :int((1 + i_segment) * segment_time * fs)])\n\n        # Check stopped\n        idx = np.logical_and(tmp >= 0, dur_tst == 0)\n        yh_tst[idx] = tmp[idx]\n        dur_tst[idx] = (1 + i_segment) * segment_time\n        if np.all(dur_tst > 0):\n            break\n\n    # Compute accuracy\n    accuracy_max_acc[i_fold] = np.mean(yh_tst == y_tst)\n    duration_max_acc[i_fold] = np.mean(dur_tst)\n\n# Compute ITR\nitr_max_acc = pyntbci.utilities.itr(n_classes, accuracy_max_acc, duration_max_acc + inter_trial_time)\n\n# Plot accuracy (over folds)\nfig, ax = plt.subplots(3, 1, figsize=(15, 8), sharex=True)\nax[0].bar(np.arange(n_folds), accuracy_max_acc)\nax[0].hlines(np.mean(accuracy_max_acc), -.5, n_folds - 0.5, linestyle='--', color=\"k\", alpha=0.5)\nax[1].bar(np.arange(n_folds), duration_max_acc)\nax[1].hlines(np.mean(duration_max_acc), -.5, n_folds - 0.5, linestyle='--', color=\"k\", alpha=0.5)\nax[2].bar(np.arange(n_folds), itr_max_acc)\nax[2].hlines(np.mean(itr_max_acc), -.5, n_folds - 0.5, linestyle='--', color=\"k\", alpha=0.5)\nax[2].set_xlabel(\"(test) fold\")\nax[0].set_ylabel(\"accuracy\")\nax[1].set_ylabel(\"duration [sec]\")\nax[2].set_ylabel(\"itr [bits/min]\")\nax[0].set_title(f\"Maximum accuracy early stopping: avg acc {accuracy_max_acc.mean():.2f} | \" +\n                f\"avg dur {duration_max_acc.mean():.2f} | avg itr {itr_max_acc.mean():.1f}\")\n\n# Print accuracy (average and standard deviation over folds)\nprint(\"Maximum accuracy:\")\nprint(f\"\\tAccuracy: avg={accuracy_max_acc.mean():.2f} with std={accuracy_max_acc.std():.2f}\")\nprint(f\"\\tDuration: avg={duration_max_acc.mean():.2f} with std={duration_max_acc.std():.2f}\")\nprint(f\"\\tITR: avg={itr_max_acc.mean():.1f} with std={itr_max_acc.std():.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Maximum ITR static stopping\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Loop folds\naccuracy_max_itr = np.zeros(n_folds)\nduration_max_itr = np.zeros(n_folds)\nfor i_fold in range(n_folds):\n\n    # Split data to train and valid set\n    X_trn, y_trn = X[folds != i_fold, :, :n_samples], y[folds != i_fold]\n    X_tst, y_tst = X[folds == i_fold, :, :n_samples], y[folds == i_fold]\n\n    # Train template-matching classifier\n    rcca = pyntbci.classifiers.rCCA(stimulus=V, fs=fs, event=\"duration\", encoding_length=encoding_length,\n                                    onset_event=onset_event, score_metric=\"correlation\")\n    max_itr = pyntbci.stopping.CriterionStopping(rcca, segment_time, fs, criterion=\"itr\", optimization=\"max\",\n                                                 smooth_width=3)\n    max_itr.fit(X_trn, y_trn)\n\n    # Loop segments\n    yh_tst = np.zeros(X_tst.shape[0])\n    dur_tst = np.zeros(X_tst.shape[0])\n    for i_segment in range(n_segments):\n\n        # Apply template-matching classifier\n        tmp = max_itr.predict(X_tst[:, :, :int((1 + i_segment) * segment_time * fs)])\n\n        # Check stopped\n        idx = np.logical_and(tmp >= 0, dur_tst == 0)\n        yh_tst[idx] = tmp[idx]\n        dur_tst[idx] = (1 + i_segment) * segment_time\n        if np.all(dur_tst > 0):\n            break\n\n    # Compute accuracy\n    accuracy_max_itr[i_fold] = np.mean(yh_tst == y_tst)\n    duration_max_itr[i_fold] = np.mean(dur_tst)\n\n# Compute ITR\nitr_max_itr = pyntbci.utilities.itr(n_classes, accuracy_max_itr, duration_max_itr + inter_trial_time)\n\n# Plot accuracy (over folds)\nfig, ax = plt.subplots(3, 1, figsize=(15, 8), sharex=True)\nax[0].bar(np.arange(n_folds), accuracy_max_itr)\nax[0].hlines(np.mean(accuracy_max_itr), -.5, n_folds - 0.5, linestyle='--', color=\"k\", alpha=0.5)\nax[1].bar(np.arange(n_folds), duration_max_itr)\nax[1].hlines(np.mean(duration_max_itr), -.5, n_folds - 0.5, linestyle='--', color=\"k\", alpha=0.5)\nax[2].bar(np.arange(n_folds), itr_max_itr)\nax[2].hlines(np.mean(itr_max_itr), -.5, n_folds - 0.5, linestyle='--', color=\"k\", alpha=0.5)\nax[2].set_xlabel(\"(test) fold\")\nax[0].set_ylabel(\"accuracy\")\nax[1].set_ylabel(\"duration [sec]\")\nax[2].set_ylabel(\"itr [bits/min]\")\nax[0].set_title(f\"Maximum ITR early stopping: avg acc {accuracy_max_itr.mean():.2f} | \" +\n                f\"avg dur {duration_max_itr.mean():.2f} | avg itr {itr_max_itr.mean():.1f}\")\n\n# Print accuracy (average and standard deviation over folds)\nprint(\"Maximum ITR:\")\nprint(f\"\\tAccuracy: avg={accuracy_max_itr.mean():.2f} with std={accuracy_max_itr.std():.2f}\")\nprint(f\"\\tDuration: avg={duration_max_itr.mean():.2f} with std={duration_max_itr.std():.2f}\")\nprint(f\"\\tITR: avg={itr_max_itr.mean():.1f} with std={itr_max_itr.std():.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Targeted accuracy static stopping\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Target accuracy\ntarget_p = 0.90 ** (1 / n_segments)\n\n# Loop folds\naccuracy_tgt_acc = np.zeros(n_folds)\nduration_tgt_acc = np.zeros(n_folds)\nfor i_fold in range(n_folds):\n\n    # Split data to train and valid set\n    X_trn, y_trn = X[folds != i_fold, :, :n_samples], y[folds != i_fold]\n    X_tst, y_tst = X[folds == i_fold, :, :n_samples], y[folds == i_fold]\n\n    # Train template-matching classifier\n    rcca = pyntbci.classifiers.rCCA(stimulus=V, fs=fs, event=\"duration\", encoding_length=encoding_length,\n                                    onset_event=onset_event, score_metric=\"correlation\")\n    tgt_acc = pyntbci.stopping.CriterionStopping(rcca, segment_time, fs, criterion=\"accuracy\", optimization=\"target\",\n                                                 target=target_p)\n    tgt_acc.fit(X_trn, y_trn)\n\n    # Loop segments\n    yh_tst = np.zeros(X_tst.shape[0])\n    dur_tst = np.zeros(X_tst.shape[0])\n    for i_segment in range(n_segments):\n\n        # Apply template-matching classifier\n        tmp = tgt_acc.predict(X_tst[:, :, :int((1 + i_segment) * segment_time * fs)])\n\n        # Check stopped\n        idx = np.logical_and(tmp >= 0, dur_tst == 0)\n        yh_tst[idx] = tmp[idx]\n        dur_tst[idx] = (1 + i_segment) * segment_time\n        if np.all(dur_tst > 0):\n            break\n\n    # Compute accuracy\n    accuracy_tgt_acc[i_fold] = np.mean(yh_tst == y_tst)\n    duration_tgt_acc[i_fold] = np.mean(dur_tst)\n\n# Compute ITR\nitr_tgt_acc = pyntbci.utilities.itr(n_classes, accuracy_tgt_acc, duration_tgt_acc + inter_trial_time)\n\n# Plot accuracy (over folds)\nfig, ax = plt.subplots(3, 1, figsize=(15, 8), sharex=True)\nax[0].bar(np.arange(n_folds), accuracy_tgt_acc)\nax[0].hlines(np.mean(accuracy_tgt_acc), -.5, n_folds - 0.5, linestyle='--', color=\"k\", alpha=0.5)\nax[1].bar(np.arange(n_folds), duration_tgt_acc)\nax[1].hlines(np.mean(duration_tgt_acc), -.5, n_folds - 0.5, linestyle='--', color=\"k\", alpha=0.5)\nax[2].bar(np.arange(n_folds), itr_tgt_acc)\nax[2].hlines(np.mean(itr_tgt_acc), -.5, n_folds - 0.5, linestyle='--', color=\"k\", alpha=0.5)\nax[2].set_xlabel(\"(test) fold\")\nax[0].set_ylabel(\"accuracy\")\nax[1].set_ylabel(\"duration [sec]\")\nax[2].set_ylabel(\"itr [bits/min]\")\nax[0].set_title(f\"Targeted accuracy early stopping: avg acc {accuracy_tgt_acc.mean():.2f} | \" +\n                f\"avg dur {duration_tgt_acc.mean():.2f} | avg itr {itr_tgt_acc.mean():.1f}\")\n\n# Print accuracy (average and standard deviation over folds)\nprint(\"Targeted accuracy:\")\nprint(f\"\\tAccuracy: avg={accuracy_tgt_acc.mean():.2f} with std={accuracy_tgt_acc.std():.2f}\")\nprint(f\"\\tDuration: avg={duration_tgt_acc.mean():.2f} with std={duration_tgt_acc.std():.2f}\")\nprint(f\"\\tITR: avg={itr_tgt_acc.mean():.1f} with std={itr_tgt_acc.std():.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Margin dynamic stopping\nThe margin method learns threshold margins (i.e., the difference between the best and second best score) to stop.\nThese margins are defined as such that a targeted accuracy is reached.\n\nReferences:\n\n.. [3] Thielen, J., van den Broek, P., Farquhar, J., & Desain, P. (2015). Broad-Band visually evoked potentials:\n       re(con)volution in brain-computer interfacing. PLOS ONE, 10(7), e0133797.\n       doi: https://doi.org/10.1371/journal.pone.0133797\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Target accuracy\ntarget_p = 0.90 ** (1 / n_segments)\n\n# Fit classifier\nrcca = pyntbci.classifiers.rCCA(stimulus=V, fs=fs, event=\"duration\", encoding_length=encoding_length,\n                                onset_event=onset_event, score_metric=\"correlation\")\nmargin = pyntbci.stopping.MarginStopping(rcca, segment_time, fs, target_p=target_p, max_time=trial_time)\nmargin.fit(X, y)\n\n# Plot dynamic stopping\nplt.figure(figsize=(15, 3))\nplt.plot(np.arange(1, 1 + margin.margins_.size) * segment_time, margin.margins_, c=\"k\")\nplt.xlabel(\"time [sec]\")\nplt.ylabel(\"margin\")\nplt.title(\"Margin dynamic stopping\")\n\n# Loop folds\naccuracy_margin = np.zeros(n_folds)\nduration_margin = np.zeros(n_folds)\nfor i_fold in range(n_folds):\n\n    # Split data to train and valid set\n    X_trn, y_trn = X[folds != i_fold, :, :n_samples], y[folds != i_fold]\n    X_tst, y_tst = X[folds == i_fold, :, :n_samples], y[folds == i_fold]\n\n    # Train template-matching classifier\n    rcca = pyntbci.classifiers.rCCA(stimulus=V, fs=fs, event=\"duration\", encoding_length=encoding_length,\n                                    onset_event=onset_event, score_metric=\"correlation\")\n    margin = pyntbci.stopping.MarginStopping(rcca, segment_time, fs, target_p=target_p)\n    margin.fit(X_trn, y_trn)\n\n    # Loop segments\n    yh_tst = np.zeros(X_tst.shape[0])\n    dur_tst = np.zeros(X_tst.shape[0])\n    for i_segment in range(n_segments):\n\n        # Apply template-matching classifier\n        tmp = margin.predict(X_tst[:, :, :int((1 + i_segment) * segment_time * fs)])\n\n        # Check stopped\n        idx = np.logical_and(tmp >= 0, dur_tst == 0)\n        yh_tst[idx] = tmp[idx]\n        dur_tst[idx] = (1 + i_segment) * segment_time\n        if np.all(dur_tst > 0):\n            break\n\n    # Compute accuracy\n    accuracy_margin[i_fold] = np.mean(yh_tst == y_tst)\n    duration_margin[i_fold] = np.mean(dur_tst)\n\n# Compute ITR\nitr_margin = pyntbci.utilities.itr(n_classes, accuracy_margin, duration_margin + inter_trial_time)\n\n# Plot accuracy (over folds)\nfig, ax = plt.subplots(3, 1, figsize=(15, 8), sharex=True)\nax[0].bar(np.arange(n_folds), accuracy_margin)\nax[0].hlines(np.mean(accuracy_margin), -.5, n_folds - 0.5, linestyle='--', color=\"k\", alpha=0.5)\nax[1].bar(np.arange(n_folds), duration_margin)\nax[1].hlines(np.mean(duration_margin), -.5, n_folds - 0.5, linestyle='--', color=\"k\", alpha=0.5)\nax[2].bar(np.arange(n_folds), itr_margin)\nax[2].hlines(np.mean(itr_margin), -.5, n_folds - 0.5, linestyle='--', color=\"k\", alpha=0.5)\nax[2].set_xlabel(\"(test) fold\")\nax[0].set_ylabel(\"accuracy\")\nax[1].set_ylabel(\"duration [sec]\")\nax[2].set_ylabel(\"itr [bits/min]\")\nax[0].set_title(f\"Margin dynamic stopping: avg acc {accuracy_margin.mean():.2f} | \" +\n                f\"avg dur {duration_margin.mean():.2f} | avg itr {itr_margin.mean():.1f}\")\n\n# Print accuracy (average and standard deviation over folds)\nprint(\"Margin:\")\nprint(f\"\\tAccuracy: avg={accuracy_margin.mean():.2f} with std={accuracy_margin.std():.2f}\")\nprint(f\"\\tDuration: avg={duration_margin.mean():.2f} with std={duration_margin.std():.2f}\")\nprint(f\"\\tITR: avg={itr_margin.mean():.1f} with std={itr_margin.std():.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Beta dynamic stopping\nThe beta method fits a beta distribution to the non-maximum scores (i.e., if correlation, then correlation+1)/2), and\ntests the probability of the maximum correlation to belong to that beta distribution.\n\nReferences:\n\n.. [4] Thielen, J., Marsman, P., Farquhar, J., & Desain, P. (2021). From full calibration to zero training for a\n       code-modulated visual evoked potentials for brain\u2013computer interface. Journal of Neural Engineering, 18(5),\n       056007. doi: http://doi.org/10.1088/1741-2552/abecef\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Target accuracy\ntarget_p = 0.90 ** (1 / n_segments)\n\n# Loop folds\naccuracy_beta = np.zeros(n_folds)\nduration_beta = np.zeros(n_folds)\nfor i_fold in range(n_folds):\n\n    # Split data to train and valid set\n    X_trn, y_trn = X[folds != i_fold, :, :n_samples], y[folds != i_fold]\n    X_tst, y_tst = X[folds == i_fold, :, :n_samples], y[folds == i_fold]\n\n    # Train template-matching classifier\n    rcca = pyntbci.classifiers.rCCA(stimulus=V, fs=fs, event=\"duration\", encoding_length=encoding_length,\n                                    onset_event=onset_event, score_metric=\"correlation\")\n    beta = pyntbci.stopping.BetaStopping(rcca, target_p=target_p, fs=fs, max_time=trial_time)\n    beta.fit(X, y)\n\n    # Loop segments\n    yh_tst = np.zeros(X_tst.shape[0])\n    dur_tst = np.zeros(X_tst.shape[0])\n    for i_segment in range(n_segments):\n\n        # Apply template-matching classifier\n        tmp = beta.predict(X_tst[:, :, :int((1 + i_segment) * segment_time * fs)])\n\n        # Check stopped\n        idx = np.logical_and(tmp >= 0, dur_tst == 0)\n        yh_tst[idx] = tmp[idx]\n        dur_tst[idx] = (1 + i_segment) * segment_time\n        if np.all(dur_tst > 0):\n            break\n\n    # Compute accuracy\n    accuracy_beta[i_fold] = np.mean(yh_tst == y_tst)\n    duration_beta[i_fold] = np.mean(dur_tst)\n\n# Compute ITR\nitr_beta = pyntbci.utilities.itr(n_classes, accuracy_beta, duration_beta + inter_trial_time)\n\n# Plot accuracy (over folds)\nfig, ax = plt.subplots(3, 1, figsize=(15, 8), sharex=True)\nax[0].bar(np.arange(n_folds), accuracy_beta)\nax[0].hlines(np.mean(accuracy_beta), -.5, n_folds - 0.5, linestyle='--', color=\"k\", alpha=0.5)\nax[1].bar(np.arange(n_folds), duration_beta)\nax[1].hlines(np.mean(duration_beta), -.5, n_folds - 0.5, linestyle='--', color=\"k\", alpha=0.5)\nax[2].bar(np.arange(n_folds), itr_beta)\nax[2].hlines(np.mean(itr_beta), -.5, n_folds - 0.5, linestyle='--', color=\"k\", alpha=0.5)\nax[2].set_xlabel(\"(test) fold\")\nax[0].set_ylabel(\"accuracy\")\nax[1].set_ylabel(\"duration [sec]\")\nax[2].set_ylabel(\"itr [bits/min]\")\nax[0].set_title(f\"Beta dynamic stopping: avg acc {accuracy_beta.mean():.2f} | \" +\n                f\"avg dur {duration_beta.mean():.2f} | avg itr {itr_beta.mean():.1f}\")\n\n# Print accuracy (average and standard deviation over folds)\nprint(\"Beta:\")\nprint(f\"\\tAccuracy: avg={accuracy_beta.mean():.2f} with std={accuracy_beta.std():.2f}\")\nprint(f\"\\tDuration: avg={duration_beta.mean():.2f} with std={duration_beta.std():.2f}\")\nprint(f\"\\tITR: avg={itr_beta.mean():.1f} with std={itr_beta.std():.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bayesian dynamic stopping (BES0)\nThe Bayesian method fits Gaussian distributions for target and non-target responses, and calculates a stopping\nthreshold using these and a cost criterion. This method comes in three flavours: bes0, bes1, and bes2.\n\nReferences:\n\n.. [5] Ahmadi, S., Thielen, J., Farquhar, J., & Desain, P. (in prep.) A model driven Bayesian dynamic stopping method\n       for parallel stimulation evoked response BCIs.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Cost ratio and target probabilities\ncr = 1.0\n\n# Fit classifier\nrcca = pyntbci.classifiers.rCCA(stimulus=V, fs=fs, event=\"duration\", encoding_length=encoding_length,\n                                onset_event=onset_event, score_metric=\"inner\")\nbayes = pyntbci.stopping.BayesStopping(rcca, segment_time, fs, cr=cr, max_time=trial_time)\nbayes.fit(X, y)\n\n# Plot dynamic stopping\nfig, ax = plt.subplots(2, 1, figsize=(15, 4), sharex=True)\nax[0].plot(np.arange(1, 1 + bayes.eta_.size) * segment_time, bayes.eta_, c=\"k\", label=\"eta\")\nax[0].plot(np.arange(1, 1 + bayes.eta_.size) * segment_time, bayes.alpha_ * bayes.b0_, \"--b\", label=\"b0\")\nax[0].plot(np.arange(1, 1 + bayes.eta_.size) * segment_time, bayes.alpha_ * bayes.b1_, \"--g\", label=\"b1\")\nax[0].plot(np.arange(1, 1 + bayes.eta_.size) * segment_time, bayes.alpha_ * bayes.b0_ - bayes.s0_, \"b\")\nax[0].plot(np.arange(1, 1 + bayes.eta_.size) * segment_time, bayes.alpha_ * bayes.b1_ - bayes.s1_, \"g\")\nax[0].plot(np.arange(1, 1 + bayes.eta_.size) * segment_time, bayes.alpha_ * bayes.b0_ + bayes.s0_, \"b\")\nax[0].plot(np.arange(1, 1 + bayes.eta_.size) * segment_time, bayes.alpha_ * bayes.b1_ + bayes.s1_, \"g\")\nax[0].legend()\nax[1].plot(np.arange(1, 1 + bayes.eta_.size) * segment_time, bayes.pf_, label=\"pf\")\nax[1].plot(np.arange(1, 1 + bayes.eta_.size) * segment_time, bayes.pm_, label=\"pm\")\nax[1].legend()\nax[1].set_xlabel(\"time [sec]\")\nax[0].set_title(\"Bayesian dynamic stopping\")\n\n# Loop folds\naccuracy_bes0 = np.zeros(n_folds)\nduration_bes0 = np.zeros(n_folds)\nfor i_fold in range(n_folds):\n\n    # Split data to train and valid set\n    X_trn, y_trn = X[folds != i_fold, :, :n_samples], y[folds != i_fold]\n    X_tst, y_tst = X[folds == i_fold, :, :n_samples], y[folds == i_fold]\n\n    # Train template-matching classifier\n    rcca = pyntbci.classifiers.rCCA(stimulus=V, fs=fs, event=\"duration\", encoding_length=encoding_length,\n                                    onset_event=onset_event, score_metric=\"inner\")\n    bayes = pyntbci.stopping.BayesStopping(rcca, segment_time, fs, method=\"bes0\", cr=cr, max_time=trial_time)\n    bayes.fit(X_trn, y_trn)\n\n    # Apply template-matching classifier\n    yh_tst = np.zeros(X_tst.shape[0])\n    dur_tst = np.zeros(X_tst.shape[0])\n    for i_segment in range(n_segments):\n        tmp = bayes.predict(X_tst[:, :, :int((1 + i_segment) * segment_time * fs)])\n        idx = np.logical_and(tmp >= 0, dur_tst == 0)\n        yh_tst[idx] = tmp[idx]\n        dur_tst[idx] = (1 + i_segment) * segment_time\n        if np.all(dur_tst > 0):\n            break\n\n    # Compute accuracy\n    accuracy_bes0[i_fold] = np.mean(yh_tst == y_tst)\n    duration_bes0[i_fold] = np.mean(dur_tst)\n\n# Compute ITR\nitr_bes0 = pyntbci.utilities.itr(n_classes, accuracy_bes0, duration_bes0 + inter_trial_time)\n\n# Plot accuracy (over folds)\nfig, ax = plt.subplots(3, 1, figsize=(15, 8), sharex=True)\nax[0].bar(np.arange(n_folds), accuracy_bes0)\nax[0].hlines(np.mean(accuracy_bes0), -.5, n_folds - 0.5, linestyle='--', color=\"k\", alpha=0.5)\nax[1].bar(np.arange(n_folds), duration_bes0)\nax[1].hlines(np.mean(duration_bes0), -.5, n_folds - 0.5, linestyle='--', color=\"k\", alpha=0.5)\nax[2].bar(np.arange(n_folds), itr_bes0)\nax[2].hlines(np.mean(itr_bes0), -.5, n_folds - 0.5, linestyle='--', color=\"k\", alpha=0.5)\nax[2].set_xlabel(\"(test) fold\")\nax[0].set_ylabel(\"accuracy\")\nax[1].set_ylabel(\"duration [sec]\")\nax[2].set_ylabel(\"itr [bits/min]\")\nax[0].set_title(f\"BES0 dynamic stopping: avg acc {accuracy_bes0.mean():.2f} | \" +\n                f\"avg dur {duration_bes0.mean():.2f} | avg itr {itr_bes0.mean():.1f}\")\n\n# Print accuracy (average and standard deviation over folds)\nprint(\"BES0:\")\nprint(f\"\\tAccuracy: avg={accuracy_bes0.mean():.2f} with std={accuracy_bes0.std():.2f}\")\nprint(f\"\\tDuration: avg={duration_bes0.mean():.2f} with std={duration_bes0.std():.2f}\")\nprint(f\"\\tITR: avg={itr_bes0.mean():.1f} with std={itr_bes0.std():.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bayesian dynamic stopping (BES1)\nThe Bayesian method fits Gaussian distributions for target and non-target responses, and calculates a stopping\nthreshold using these and a cost criterion. This method comes in three flavours: bes0, bes1, and bes2.\n\nReferences:\n\n.. [6] Ahmadi, S., Thielen, J., Farquhar, J., & Desain, P. (in prep.) A model driven Bayesian dynamic stopping method\n       for parallel stimulation evoked response BCIs.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Cost ratio and target probabilities\ncr = 1.0\ntarget_pf = 0.05\ntarget_pd = 0.80\n\n# Loop folds\naccuracy_bes1 = np.zeros(n_folds)\nduration_bes1 = np.zeros(n_folds)\nfor i_fold in range(n_folds):\n\n    # Split data to train and valid set\n    X_trn, y_trn = X[folds != i_fold, :, :n_samples], y[folds != i_fold]\n    X_tst, y_tst = X[folds == i_fold, :, :n_samples], y[folds == i_fold]\n\n    # Train template-matching classifier\n    rcca = pyntbci.classifiers.rCCA(stimulus=V, fs=fs, event=\"duration\", encoding_length=encoding_length,\n                                    onset_event=onset_event, score_metric=\"inner\")\n    bayes = pyntbci.stopping.BayesStopping(rcca, segment_time, fs, method=\"bes1\", cr=cr, target_pf=target_pf,\n                                           target_pd=target_pd, max_time=trial_time)\n    bayes.fit(X_trn, y_trn)\n\n    # Apply template-matching classifier\n    yh_tst = np.zeros(X_tst.shape[0])\n    dur_tst = np.zeros(X_tst.shape[0])\n    for i_segment in range(n_segments):\n        tmp = bayes.predict(X_tst[:, :, :int((1 + i_segment) * segment_time * fs)])\n        idx = np.logical_and(tmp >= 0, dur_tst == 0)\n        yh_tst[idx] = tmp[idx]\n        dur_tst[idx] = (1 + i_segment) * segment_time\n        if np.all(dur_tst > 0):\n            break\n\n    # Compute accuracy\n    accuracy_bes1[i_fold] = np.mean(yh_tst == y_tst)\n    duration_bes1[i_fold] = np.mean(dur_tst)\n\n# Compute ITR\nitr_bes1 = pyntbci.utilities.itr(n_classes, accuracy_bes1, duration_bes1 + inter_trial_time)\n\n# Plot accuracy (over folds)\nfig, ax = plt.subplots(3, 1, figsize=(15, 8), sharex=True)\nax[0].bar(np.arange(n_folds), accuracy_bes1)\nax[0].hlines(np.mean(accuracy_bes1), -.5, n_folds - 0.5, linestyle='--', color=\"k\", alpha=0.5)\nax[1].bar(np.arange(n_folds), duration_bes1)\nax[1].hlines(np.mean(duration_bes1), -.5, n_folds - 0.5, linestyle='--', color=\"k\", alpha=0.5)\nax[2].bar(np.arange(n_folds), itr_bes1)\nax[2].hlines(np.mean(itr_bes1), -.5, n_folds - 0.5, linestyle='--', color=\"k\", alpha=0.5)\nax[2].set_xlabel(\"(test) fold\")\nax[0].set_ylabel(\"accuracy\")\nax[1].set_ylabel(\"duration [sec]\")\nax[2].set_ylabel(\"itr [bits/min]\")\nax[0].set_title(f\"BES1 dynamic stopping: avg acc {accuracy_bes1.mean():.2f} | \" +\n                f\"avg dur {duration_bes1.mean():.2f} | avg itr {itr_bes1.mean():.1f}\")\n\n# Print accuracy (average and standard deviation over folds)\nprint(\"BES1:\")\nprint(f\"\\tAccuracy: avg={accuracy_bes1.mean():.2f} with std={accuracy_bes1.std():.2f}\")\nprint(f\"\\tDuration: avg={duration_bes1.mean():.2f} with std={duration_bes1.std():.2f}\")\nprint(f\"\\tITR: avg={itr_bes1.mean():.1f} with std={itr_bes1.std():.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bayesian dynamic stopping (BES2)\nThe Bayesian method fits Gaussian distributions for target and non-target responses, and calculates a stopping\nthreshold using these and a cost criterion. This method comes in three flavours: bes0, bes1, and bes2.\n\nReferences:\n\n.. [7] Ahmadi, S., Thielen, J., Farquhar, J., & Desain, P. (in prep.) A model driven Bayesian dynamic stopping method\n       for parallel stimulation evoked response BCIs.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Cost ratio and target probabilities\ncr = 1.0\ntarget_pf = 0.05\ntarget_pd = 0.80\n\n# Loop folds\naccuracy_bes2 = np.zeros(n_folds)\nduration_bes2 = np.zeros(n_folds)\nfor i_fold in range(n_folds):\n\n    # Split data to train and valid set\n    X_trn, y_trn = X[folds != i_fold, :, :n_samples], y[folds != i_fold]\n    X_tst, y_tst = X[folds == i_fold, :, :n_samples], y[folds == i_fold]\n\n    # Train template-matching classifier\n    rcca = pyntbci.classifiers.rCCA(stimulus=V, fs=fs, event=\"duration\", encoding_length=encoding_length,\n                                    onset_event=onset_event, score_metric=\"inner\")\n    bayes = pyntbci.stopping.BayesStopping(rcca, segment_time, fs, method=\"bes2\", cr=cr, target_pf=target_pf,\n                                           target_pd=target_pd, max_time=trial_time)\n    bayes.fit(X_trn, y_trn)\n\n    # Apply template-matching classifier\n    yh_tst = np.zeros(X_tst.shape[0])\n    dur_tst = np.zeros(X_tst.shape[0])\n    for i_segment in range(n_segments):\n        tmp = bayes.predict(X_tst[:, :, :int((1 + i_segment) * segment_time * fs)])\n        idx = np.logical_and(tmp >= 0, dur_tst == 0)\n        yh_tst[idx] = tmp[idx]\n        dur_tst[idx] = (1 + i_segment) * segment_time\n        if np.all(dur_tst > 0):\n            break\n\n    # Compute accuracy\n    accuracy_bes2[i_fold] = np.mean(yh_tst == y_tst)\n    duration_bes2[i_fold] = np.mean(dur_tst)\n\n# Compute ITR\nitr_bes2 = pyntbci.utilities.itr(n_classes, accuracy_bes2, duration_bes2 + inter_trial_time)\n\n# Plot accuracy (over folds)\nfig, ax = plt.subplots(3, 1, figsize=(15, 8), sharex=True)\nax[0].bar(np.arange(n_folds), accuracy_bes2)\nax[0].hlines(np.mean(accuracy_bes2), -.5, n_folds - 0.5, linestyle='--', color=\"k\", alpha=0.5)\nax[1].bar(np.arange(n_folds), duration_bes2)\nax[1].hlines(np.mean(duration_bes2), -.5, n_folds - 0.5, linestyle='--', color=\"k\", alpha=0.5)\nax[2].bar(np.arange(n_folds), itr_bes2)\nax[2].hlines(np.mean(itr_bes2), -.5, n_folds - 0.5, linestyle='--', color=\"k\", alpha=0.5)\nax[2].set_xlabel(\"(test) fold\")\nax[0].set_ylabel(\"accuracy\")\nax[1].set_ylabel(\"duration [sec]\")\nax[2].set_ylabel(\"itr [bits/min]\")\nax[0].set_title(f\"BES2 dynamic stopping: avg acc {accuracy_bes2.mean():.2f} | \" +\n                f\"avg dur {duration_bes2.mean():.2f} | avg itr {itr_bes2.mean():.1f}\")\n\n# Print accuracy (average and standard deviation over folds)\nprint(\"BES2:\")\nprint(f\"\\tAccuracy: avg={accuracy_bes2.mean():.2f} with std={accuracy_bes2.std():.2f}\")\nprint(f\"\\tDuration: avg={duration_bes2.mean():.2f} with std={duration_bes2.std():.2f}\")\nprint(f\"\\tITR: avg={itr_bes2.mean():.1f} with std={itr_bes2.std():.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Overall comparison\nComparison of the presented stopping methods. Note, each of these use default parameters that might need fine-tuning.\nAdditionally, the evaluation is performed on a single participant only.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Plot accuracy\nwidth = 0.8\nfig, ax = plt.subplots(3, 1, figsize=(15, 8), sharex=True)\nax[0].bar(0, accuracy_max_acc.mean(), width=width, yerr=accuracy_max_acc.std(), label=\"maxacc\")\nax[0].bar(1, accuracy_max_itr.mean(), width=width, yerr=accuracy_max_itr.std(), label=\"maxitr\")\nax[0].bar(2, accuracy_tgt_acc.mean(), width=width, yerr=accuracy_tgt_acc.std(), label=\"tgtacc\")\nax[0].bar(3, accuracy_margin.mean(), width=width, yerr=accuracy_margin.std(), label=\"margin\")\nax[0].bar(4, accuracy_beta.mean(), width=width, yerr=accuracy_beta.std(), label=\"beta\")\nax[0].bar(5, accuracy_bes0.mean(), width=width, yerr=accuracy_bes0.std(), label=\"bes0\")\nax[0].bar(6, accuracy_bes1.mean(), width=width, yerr=accuracy_bes1.std(), label=\"bes1\")\nax[0].bar(7, accuracy_bes2.mean(), width=width, yerr=accuracy_bes2.std(), label=\"bes2\")\nax[1].bar(0, duration_max_acc.mean(), width=width, yerr=duration_max_acc.std(), label=\"maxacc\")\nax[1].bar(1, duration_max_itr.mean(), width=width, yerr=duration_max_itr.std(), label=\"maxitr\")\nax[1].bar(2, duration_tgt_acc.mean(), width=width, yerr=duration_tgt_acc.std(), label=\"tgtacc\")\nax[1].bar(3, duration_margin.mean(), width=width, yerr=duration_margin.std(), label=\"margin\")\nax[1].bar(4, duration_beta.mean(), width=width, yerr=duration_beta.std(), label=\"beta\")\nax[1].bar(5, duration_bes0.mean(), width=width, yerr=duration_bes0.std(), label=\"bes0\")\nax[1].bar(6, duration_bes1.mean(), width=width, yerr=duration_bes1.std(), label=\"bes1\")\nax[1].bar(7, duration_bes2.mean(), width=width, yerr=duration_bes2.std(), label=\"bes2\")\nax[2].bar(0, itr_max_acc.mean(), width=width, yerr=itr_max_acc.std(), label=\"maxacc\")\nax[2].bar(1, itr_max_itr.mean(), width=width, yerr=itr_max_itr.std(), label=\"maxitr\")\nax[2].bar(2, itr_tgt_acc.mean(), width=width, yerr=itr_tgt_acc.std(), label=\"tgtacc\")\nax[2].bar(3, itr_margin.mean(), width=width, yerr=itr_margin.std(), label=\"margin\")\nax[2].bar(4, itr_beta.mean(), width=width, yerr=itr_beta.std(), label=\"beta\")\nax[2].bar(5, itr_bes0.mean(), width=width, yerr=itr_bes0.std(), label=\"bes0\")\nax[2].bar(6, itr_bes1.mean(), width=width, yerr=itr_bes1.std(), label=\"bes1\")\nax[2].bar(7, itr_bes2.mean(), width=width, yerr=itr_bes2.std(), label=\"bes2\")\nax[2].set_xticks(np.arange(8), [\"maxacc\", \"maxitr\", \"tgtacc\", \"margin\", \"beta\", \"bes0\", \"bes1\", \"bes2\"])\nax[2].set_xlabel(\"early stopping method\")\nax[0].set_ylabel(\"accuracy\")\nax[1].set_ylabel(\"duration [sec]\")\nax[2].set_ylabel(\"itr [bits/min]\")\nax[1].legend(bbox_to_anchor=(1.0, 1.0))\nax[0].set_title(\"Comparison of early stopping methods averaged across folds\")\n\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}